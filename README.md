# Survey

<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2402.17302v2
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2403.14112v2
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2502.11393v1
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2502.11393v1
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2502.11393v1
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2502.11393v1
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2502.11393v1
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2502.11393v1
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2502.11393v1
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2509.06401v1
Do LLMs exhibit the same commonsense capabilities across languages?
This paper explores the multilingual commonsense generation abilities of
Large Language Models (LLMs). To facilitate this investigation, we introduce
MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four
languages: English, Spanish, Dutch, and Valencian. The task involves generating
a commonsensical sentence that includes a given triplet of words. We evaluate a
range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and
Salamandra, on this benchmark. Our evaluation combines automatic metrics,
LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human
annotations. Results consistently show superior performance in English, with
significantly lower performance in less-resourced languages. While contextual
support yields mixed results, it tends to benefit underrepresented languages.
These findings underscore the current limitations of LLMs in multilingual
commonsense generation. The dataset is publicly available at
https://huggingface.co/datasets/gplsi/MULTICOM.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2509.06401v1
Do LLMs exhibit the same commonsense capabilities across languages?
This paper explores the multilingual commonsense generation abilities of
Large Language Models (LLMs). To facilitate this investigation, we introduce
MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four
languages: English, Spanish, Dutch, and Valencian. The task involves generating
a commonsensical sentence that includes a given triplet of words. We evaluate a
range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and
Salamandra, on this benchmark. Our evaluation combines automatic metrics,
LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human
annotations. Results consistently show superior performance in English, with
significantly lower performance in less-resourced languages. While contextual
support yields mixed results, it tends to benefit underrepresented languages.
These findings underscore the current limitations of LLMs in multilingual
commonsense generation. The dataset is publicly available at
https://huggingface.co/datasets/gplsi/MULTICOM.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2509.06401v1
Do LLMs exhibit the same commonsense capabilities across languages?
This paper explores the multilingual commonsense generation abilities of
Large Language Models (LLMs). To facilitate this investigation, we introduce
MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four
languages: English, Spanish, Dutch, and Valencian. The task involves generating
a commonsensical sentence that includes a given triplet of words. We evaluate a
range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and
Salamandra, on this benchmark. Our evaluation combines automatic metrics,
LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human
annotations. Results consistently show superior performance in English, with
significantly lower performance in less-resourced languages. While contextual
support yields mixed results, it tends to benefit underrepresented languages.
These findings underscore the current limitations of LLMs in multilingual
commonsense generation. The dataset is publicly available at
https://huggingface.co/datasets/gplsi/MULTICOM.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2509.24468v1
Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset
Large language models (LLMs) exhibit social biases, prompting the development
of various debiasing methods. However, debiasing methods may degrade the
capabilities of LLMs. Previous research has evaluated the impact of bias
mitigation primarily through tasks measuring general language understanding,
which are often unrelated to social biases. In contrast, cultural commonsense
is closely related to social biases, as both are rooted in social norms and
values. The impact of bias mitigation on cultural commonsense in LLMs has not
been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs
and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate
social biases and cultural commonsense in LLMs in a unified format. We evaluate
several LLMs on SOBACO to examine how debiasing methods affect cultural
commonsense in LLMs. Our results reveal that the debiasing methods degrade the
performance of the LLMs on the cultural commonsense task (up to 75% accuracy
deterioration). These results highlight the importance of developing debiasing
methods that consider the trade-off with cultural commonsense to improve
fairness and utility of LLMs.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2503.13445v1
Faithfulness of LLM Self-Explanations for Commonsense Tasks: Larger Is Better, and Instruction-Tuning Allows Trade-Offs but Not Pareto Dominance
As large language models (LLMs) become increasingly capable, ensuring that
their self-generated explanations are faithful to their internal
decision-making process is critical for safety and oversight. In this work, we
conduct a comprehensive counterfactual faithfulness analysis across 62 models
from 8 families, encompassing both pretrained and instruction-tuned variants
and significantly extending prior studies of counterfactual tests. We introduce
phi-CCT, a simplified variant of the Correlational Counterfactual Test, which
avoids the need for token probabilities while explaining most of the variance
of the original test. Our findings reveal clear scaling trends: larger models
are consistently more faithful on our metrics. However, when comparing
instruction-tuned and human-imitated explanations, we find that observed
differences in faithfulness can often be attributed to explanation verbosity,
leading to shifts along the true-positive/false-positive Pareto frontier. While
instruction-tuning and prompting can influence this trade-off, we find limited
evidence that they fundamentally expand the frontier of explanatory
faithfulness beyond what is achievable with pretrained models of comparable
size. Our analysis highlights the nuanced relationship between
instruction-tuning, verbosity, and the faithful representation of model
decision processes.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2509.24468v1
Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset
Large language models (LLMs) exhibit social biases, prompting the development
of various debiasing methods. However, debiasing methods may degrade the
capabilities of LLMs. Previous research has evaluated the impact of bias
mitigation primarily through tasks measuring general language understanding,
which are often unrelated to social biases. In contrast, cultural commonsense
is closely related to social biases, as both are rooted in social norms and
values. The impact of bias mitigation on cultural commonsense in LLMs has not
been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs
and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate
social biases and cultural commonsense in LLMs in a unified format. We evaluate
several LLMs on SOBACO to examine how debiasing methods affect cultural
commonsense in LLMs. Our results reveal that the debiasing methods degrade the
performance of the LLMs on the cultural commonsense task (up to 75% accuracy
deterioration). These results highlight the importance of developing debiasing
methods that consider the trade-off with cultural commonsense to improve
fairness and utility of LLMs.
<br>http://arxiv.org/abs/2509.24468v1
Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset
Large language models (LLMs) exhibit social biases, prompting the development
of various debiasing methods. However, debiasing methods may degrade the
capabilities of LLMs. Previous research has evaluated the impact of bias
mitigation primarily through tasks measuring general language understanding,
which are often unrelated to social biases. In contrast, cultural commonsense
is closely related to social biases, as both are rooted in social norms and
values. The impact of bias mitigation on cultural commonsense in LLMs has not
been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs
and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate
social biases and cultural commonsense in LLMs in a unified format. We evaluate
several LLMs on SOBACO to examine how debiasing methods affect cultural
commonsense in LLMs. Our results reveal that the debiasing methods degrade the
performance of the LLMs on the cultural commonsense task (up to 75% accuracy
deterioration). These results highlight the importance of developing debiasing
methods that consider the trade-off with cultural commonsense to improve
fairness and utility of LLMs.
<br>http://arxiv.org/abs/2509.06401v1
Do LLMs exhibit the same commonsense capabilities across languages?
This paper explores the multilingual commonsense generation abilities of
Large Language Models (LLMs). To facilitate this investigation, we introduce
MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four
languages: English, Spanish, Dutch, and Valencian. The task involves generating
a commonsensical sentence that includes a given triplet of words. We evaluate a
range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and
Salamandra, on this benchmark. Our evaluation combines automatic metrics,
LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human
annotations. Results consistently show superior performance in English, with
significantly lower performance in less-resourced languages. While contextual
support yields mixed results, it tends to benefit underrepresented languages.
These findings underscore the current limitations of LLMs in multilingual
commonsense generation. The dataset is publicly available at
https://huggingface.co/datasets/gplsi/MULTICOM.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and
timeliness of large language models (LLMs) . However, the setting of this task
overlooks a significant portion of commonsense knowledge based on free-text in
the real world, characterized by broad knowledge scope, long content and non
instantiation. The editing objects of previous methods (e.g., MEMIT) were
single token or entity, which were not suitable for commonsense knowledge in
free-text form. To address the aforementioned challenges, we conducted
experiments from two perspectives: knowledge localization and knowledge
editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT)
method, revealing the challenges associated with the distribution of
commonsense knowledge in MLP and Attention layers, as well as in decentralized
distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which
utilizes a Dynamics-aware Module to locate the parameter positions
corresponding to commonsense knowledge, and uses Knowledge Editing Module to
update knowledge. The DEM method fully explores the potential of the MLP and
Attention layers, and successfully edits commonsense knowledge based on
free-text. The experimental results indicate that the DEM can achieve excellent
editing performance.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been
made about their abilities, including for commonsense reasoning. Given the
increasingly better results of current language models on previous static
benchmarks for commonsense reasoning, we explore an alternative dialectical
evaluation. The goal of this kind of evaluation is not to obtain an aggregate
performance value but to find failures and map the boundaries of the system.
Dialoguing with the system gives the opportunity to check for consistency and
get more reassurance of these boundaries beyond anecdotal evidence. In this
paper we conduct some qualitative investigations of this kind of evaluation for
the particular case of spatial reasoning (which is a fundamental aspect of
commonsense reasoning). We conclude with some suggestions for future work both
to improve the capabilities of language models and to systematise this kind of
dialectical evaluation.
<br>http://arxiv.org/abs/2509.24468v1
Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset
Large language models (LLMs) exhibit social biases, prompting the development
of various debiasing methods. However, debiasing methods may degrade the
capabilities of LLMs. Previous research has evaluated the impact of bias
mitigation primarily through tasks measuring general language understanding,
which are often unrelated to social biases. In contrast, cultural commonsense
is closely related to social biases, as both are rooted in social norms and
values. The impact of bias mitigation on cultural commonsense in LLMs has not
been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs
and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate
social biases and cultural commonsense in LLMs in a unified format. We evaluate
several LLMs on SOBACO to examine how debiasing methods affect cultural
commonsense in LLMs. Our results reveal that the debiasing methods degrade the
performance of the LLMs on the cultural commonsense task (up to 75% accuracy
deterioration). These results highlight the importance of developing debiasing
methods that consider the trade-off with cultural commonsense to improve
fairness and utility of LLMs.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in
answering questions pertaining to natural language inference and commonsense
reasoning. Composing these questions is itself a skilled writing task, so in
this paper we consider LLMs as authors of commonsense assessment items. We
prompt LLMs to generate items in the style of a prominent benchmark for
commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine
the outcome according to analyses facilitated by the LLMs and human annotation.
We find that LLMs that succeed in answering the original COPA benchmark are
also more successful in authoring their own items.
<br>http://arxiv.org/abs/2509.18063v1
ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning
Large Language Models (LLMs) show strong reasoning abilities but rely on
internalized knowledge that is often insufficient, outdated, or incorrect when
trying to answer a question that requires specific domain knowledge. Knowledge
Graphs (KGs) provide structured external knowledge, yet their complexity and
multi-hop reasoning requirements make integration challenging. We present
ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural
language queries. We evaluate several not fine-tuned state-of-the art LLMs as
backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and
commonsense reasoning over long-tail entities. ARK-V1 achieves substantially
higher conditional accuracies than Chain-of-Thought baselines, and larger
backbone models show a clear trend toward better coverage, correctness, and
stability.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2509.24468v1
Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset
Large language models (LLMs) exhibit social biases, prompting the development
of various debiasing methods. However, debiasing methods may degrade the
capabilities of LLMs. Previous research has evaluated the impact of bias
mitigation primarily through tasks measuring general language understanding,
which are often unrelated to social biases. In contrast, cultural commonsense
is closely related to social biases, as both are rooted in social norms and
values. The impact of bias mitigation on cultural commonsense in LLMs has not
been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs
and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate
social biases and cultural commonsense in LLMs in a unified format. We evaluate
several LLMs on SOBACO to examine how debiasing methods affect cultural
commonsense in LLMs. Our results reveal that the debiasing methods degrade the
performance of the LLMs on the cultural commonsense task (up to 75% accuracy
deterioration). These results highlight the importance of developing debiasing
methods that consider the trade-off with cultural commonsense to improve
fairness and utility of LLMs.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2509.19265v1
Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World
Large language models (LLMs) often reflect Western-centric biases, limiting
their effectiveness in diverse cultural contexts. Although some work has
explored cultural alignment, the potential for cross-cultural transfer, using
alignment in one culture to improve performance in others, remains
underexplored. This paper investigates cross-cultural transfer of commonsense
reasoning in the Arab world, where linguistic and historical similarities
coexist with local cultural differences. Using a culturally grounded
commonsense reasoning dataset covering 13 Arab countries, we evaluate
lightweight alignment methods such as in-context learning and
demonstration-based reinforcement (DITTO), alongside baselines like supervised
fine-tuning and direct preference optimization. Our results show that merely 12
culture-specific examples from one country can improve performance in others by
10\% on average, within multilingual models. In addition, we demonstrate that
out-of-culture demonstrations from Indonesia and US contexts can match or
surpass in-culture alignment for MCQ reasoning, highlighting cultural
commonsense transferability beyond the Arab world. These findings demonstrate
that efficient cross-cultural alignment is possible and offer a promising
approach to adapt LLMs to low-resource cultural settings.
<br>http://arxiv.org/abs/2509.19265v1
Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World
Large language models (LLMs) often reflect Western-centric biases, limiting
their effectiveness in diverse cultural contexts. Although some work has
explored cultural alignment, the potential for cross-cultural transfer, using
alignment in one culture to improve performance in others, remains
underexplored. This paper investigates cross-cultural transfer of commonsense
reasoning in the Arab world, where linguistic and historical similarities
coexist with local cultural differences. Using a culturally grounded
commonsense reasoning dataset covering 13 Arab countries, we evaluate
lightweight alignment methods such as in-context learning and
demonstration-based reinforcement (DITTO), alongside baselines like supervised
fine-tuning and direct preference optimization. Our results show that merely 12
culture-specific examples from one country can improve performance in others by
10\% on average, within multilingual models. In addition, we demonstrate that
out-of-culture demonstrations from Indonesia and US contexts can match or
surpass in-culture alignment for MCQ reasoning, highlighting cultural
commonsense transferability beyond the Arab world. These findings demonstrate
that efficient cross-cultural alignment is possible and offer a promising
approach to adapt LLMs to low-resource cultural settings.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2509.18063v1
ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning
Large Language Models (LLMs) show strong reasoning abilities but rely on
internalized knowledge that is often insufficient, outdated, or incorrect when
trying to answer a question that requires specific domain knowledge. Knowledge
Graphs (KGs) provide structured external knowledge, yet their complexity and
multi-hop reasoning requirements make integration challenging. We present
ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural
language queries. We evaluate several not fine-tuned state-of-the art LLMs as
backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and
commonsense reasoning over long-tail entities. ARK-V1 achieves substantially
higher conditional accuracies than Chain-of-Thought baselines, and larger
backbone models show a clear trend toward better coverage, correctness, and
stability.
<br>http://arxiv.org/abs/2509.06401v1
Do LLMs exhibit the same commonsense capabilities across languages?
This paper explores the multilingual commonsense generation abilities of
Large Language Models (LLMs). To facilitate this investigation, we introduce
MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four
languages: English, Spanish, Dutch, and Valencian. The task involves generating
a commonsensical sentence that includes a given triplet of words. We evaluate a
range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and
Salamandra, on this benchmark. Our evaluation combines automatic metrics,
LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human
annotations. Results consistently show superior performance in English, with
significantly lower performance in less-resourced languages. While contextual
support yields mixed results, it tends to benefit underrepresented languages.
These findings underscore the current limitations of LLMs in multilingual
commonsense generation. The dataset is publicly available at
https://huggingface.co/datasets/gplsi/MULTICOM.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth
evaluating the commonsense reasoning ability of large language models (LLMs) in
Chinese, which covers both globally known and Chinese-specific commonsense. We
evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5
representative prompt strategies for improving LLMs' reasoning ability, such as
Chain-of-Thought. Our findings indicate that the LLM's language orientation and
the task's domain influence the effectiveness of the prompt strategy, which
enriches previous research findings. We built closely-interconnected reasoning
and memorization tasks, and found that some LLMs struggle with memorizing
Chinese commonsense, affecting their reasoning ability, while others show
differences in reasoning despite similar memorization performance. We also
evaluated the LLMs' memorization-independent reasoning abilities and analyzed
the typical errors. Our study precisely identified the LLMs' strengths and
weaknesses, providing the clear direction for optimization. It can also serve
as a reference for studies in other fields. We will release CHARM at
https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators, resulting in ~4.5K questions per language (~9K in total), making
our dataset the largest of its kind. Our experiments show that automatic data
adaptation from an existing English dataset is less effective for Sundanese.
Interestingly, using the direct generation method on the target language, GPT-4
Turbo can generate questions with adequate general knowledge in both languages,
albeit not as culturally 'deep' as humans. We also observe a higher occurrence
of fluency errors in the Sundanese dataset, highlighting the discrepancy
between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2509.18063v1
ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning
Large Language Models (LLMs) show strong reasoning abilities but rely on
internalized knowledge that is often insufficient, outdated, or incorrect when
trying to answer a question that requires specific domain knowledge. Knowledge
Graphs (KGs) provide structured external knowledge, yet their complexity and
multi-hop reasoning requirements make integration challenging. We present
ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural
language queries. We evaluate several not fine-tuned state-of-the art LLMs as
backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and
commonsense reasoning over long-tail entities. ARK-V1 achieves substantially
higher conditional accuracies than Chain-of-Thought baselines, and larger
backbone models show a clear trend toward better coverage, correctness, and
stability.
<br>http://arxiv.org/abs/2509.24468v1
Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset
Large language models (LLMs) exhibit social biases, prompting the development
of various debiasing methods. However, debiasing methods may degrade the
capabilities of LLMs. Previous research has evaluated the impact of bias
mitigation primarily through tasks measuring general language understanding,
which are often unrelated to social biases. In contrast, cultural commonsense
is closely related to social biases, as both are rooted in social norms and
values. The impact of bias mitigation on cultural commonsense in LLMs has not
been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs
and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate
social biases and cultural commonsense in LLMs in a unified format. We evaluate
several LLMs on SOBACO to examine how debiasing methods affect cultural
commonsense in LLMs. Our results reveal that the debiasing methods degrade the
performance of the LLMs on the cultural commonsense task (up to 75% accuracy
deterioration). These results highlight the importance of developing debiasing
methods that consider the trade-off with cultural commonsense to improve
fairness and utility of LLMs.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2509.18063v1
ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning
Large Language Models (LLMs) show strong reasoning abilities but rely on
internalized knowledge that is often insufficient, outdated, or incorrect when
trying to answer a question that requires specific domain knowledge. Knowledge
Graphs (KGs) provide structured external knowledge, yet their complexity and
multi-hop reasoning requirements make integration challenging. We present
ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural
language queries. We evaluate several not fine-tuned state-of-the art LLMs as
backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and
commonsense reasoning over long-tail entities. ARK-V1 achieves substantially
higher conditional accuracies than Chain-of-Thought baselines, and larger
backbone models show a clear trend toward better coverage, correctness, and
stability.
<br>http://arxiv.org/abs/2509.24468v1
Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset
Large language models (LLMs) exhibit social biases, prompting the development
of various debiasing methods. However, debiasing methods may degrade the
capabilities of LLMs. Previous research has evaluated the impact of bias
mitigation primarily through tasks measuring general language understanding,
which are often unrelated to social biases. In contrast, cultural commonsense
is closely related to social biases, as both are rooted in social norms and
values. The impact of bias mitigation on cultural commonsense in LLMs has not
been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs
and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate
social biases and cultural commonsense in LLMs in a unified format. We evaluate
several LLMs on SOBACO to examine how debiasing methods affect cultural
commonsense in LLMs. Our results reveal that the debiasing methods degrade the
performance of the LLMs on the cultural commonsense task (up to 75% accuracy
deterioration). These results highlight the importance of developing debiasing
methods that consider the trade-off with cultural commonsense to improve
fairness and utility of LLMs.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in
commonsense reasoning; however, some variations in questions can trigger
incorrect responses. Do these models truly understand commonsense knowledge, or
just memorize expression patterns? To investigate this question, we present the
first extensive robustness evaluation of LLMs in commonsense reasoning. We
introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200
cases, by designing and compiling seven types of question variants. To
construct this benchmark, we propose a two-stage method to develop Chinese
HellaSwag, a finely annotated dataset comprising 12,000 instances across 56
categories. We conduct extensive experiments on 41 representative LLMs,
revealing that these LLMs are far from robust in commonsense reasoning.
Furthermore, this robustness varies depending on the language in which the LLM
is tested. This work establishes a high-quality evaluation benchmark, with
extensive experiments offering valuable insights to the community in
commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2511.04087v1
E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce
Finding relevant products given a user query plays a pivotal role in an
e-commerce platform, as it can spark shopping behaviors and result in revenue
gains. The challenge lies in accurately predicting the correlation between
queries and products. Recently, mining the cross-features between queries and
products based on the commonsense reasoning capacity of Large Language Models
(LLMs) has shown promising performance. However, such methods suffer from high
costs due to intensive real-time LLM inference during serving, as well as human
annotations and potential Supervised Fine Tuning (SFT). To boost efficiency
while leveraging the commonsense reasoning capacity of LLMs for various
e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation
Enhancer (E-CARE). During inference, models augmented with E-CARE can access
commonsense reasoning with only a single LLM forward pass per query by
utilizing a commonsense reasoning factor graph that encodes most of the
reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show
an improvement of up to 12.1% on precision@5.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by
disrupting environmental understanding. Current protocols typically respond
with immediate stops or minimal-risk maneuvers, worsening traffic flow and
lacking flexibility for rare driving scenarios. In this paper, we propose
LLM-RCO, a framework leveraging large language models to integrate human-like
driving commonsense into autonomous systems facing perception deficits. LLM-RCO
features four key modules: hazard inference, short-term motion planner, action
condition verifier, and safety constraint generator. These modules interact
with the dynamic driving environment, enabling proactive and context-aware
control actions to override the original control policy of autonomous agents.
To improve safety in such challenging conditions, we construct DriveLM-Deficit,
a dataset of 53,895 video clips featuring deficits of safety-critical objects,
complete with annotations for LLM-based hazard inference and motion planning
fine-tuning. Extensive experiments in adverse driving conditions with the CARLA
simulator demonstrate that systems equipped with LLM-RCO significantly improve
driving performance, highlighting its potential for enhancing autonomous
driving resilience against adverse perception deficits. Our results also show
that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements
instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant
commonsense persona knowledge, but retrieving this knowledge is challenging due
to complex contexts and the implicit nature of commonsense. This paper presents
our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge,
addressing the critical need for integrating persona and commonsense knowledge
in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that
leverages Large Language Models to generate high-quality synthetic datasets for
training commonsense persona knowledge linkers. To demonstrate the efficacy of
our approach, we present SynCPKL, a new dataset specifically designed for this
task. Our experiments validate the effectiveness of SynCPKL for training
commonsense persona knowledge linkers. Additionally, our top-performing model,
Derberta-SynCPKL, secured first place in the CPKL challenge by a 16%
improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at
https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2509.24468v1
Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset
Large language models (LLMs) exhibit social biases, prompting the development
of various debiasing methods. However, debiasing methods may degrade the
capabilities of LLMs. Previous research has evaluated the impact of bias
mitigation primarily through tasks measuring general language understanding,
which are often unrelated to social biases. In contrast, cultural commonsense
is closely related to social biases, as both are rooted in social norms and
values. The impact of bias mitigation on cultural commonsense in LLMs has not
been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs
and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate
social biases and cultural commonsense in LLMs in a unified format. We evaluate
several LLMs on SOBACO to examine how debiasing methods affect cultural
commonsense in LLMs. Our results reveal that the debiasing methods degrade the
performance of the LLMs on the cultural commonsense task (up to 75% accuracy
deterioration). These results highlight the importance of developing debiasing
methods that consider the trade-off with cultural commonsense to improve
fairness and utility of LLMs.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and timeliness of large language models (LLMs) . However, the setting of this task overlooks a significant portion of commonsense knowledge based on free-text in the real world, characterized by broad knowledge scope, long content and non instantiation. The editing objects of previous methods (e.g., MEMIT) were single token or entity, which were not suitable for commonsense knowledge in free-text form. To address the aforementioned challenges, we conducted experiments from two perspectives: knowledge localization and knowledge editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT) method, revealing the challenges associated with the distribution of commonsense knowledge in MLP and Attention layers, as well as in decentralized distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which utilizes a Dynamics-aware Module to locate the parameter positions corresponding to commonsense knowledge, and uses Knowledge Editing Module to update knowledge. The DEM method fully explores the potential of the MLP and Attention layers, and successfully edits commonsense knowledge based on free-text. The experimental results indicate that the DEM can achieve excellent editing performance.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been made about their abilities, including for commonsense reasoning. Given the increasingly better results of current language models on previous static benchmarks for commonsense reasoning, we explore an alternative dialectical evaluation. The goal of this kind of evaluation is not to obtain an aggregate performance value but to find failures and map the boundaries of the system. Dialoguing with the system gives the opportunity to check for consistency and get more reassurance of these boundaries beyond anecdotal evidence. In this paper we conduct some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning). We conclude with some suggestions for future work both to improve the capabilities of language models and to systematise this kind of dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in ~4.5K questions per language (~9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally 'deep' as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in ~4.5K questions per language (~9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally 'deep' as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2509.06401v1
Do LLMs exhibit the same commonsense capabilities across languages?
This paper explores the multilingual commonsense generation abilities of Large Language Models (LLMs). To facilitate this investigation, we introduce MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four languages: English, Spanish, Dutch, and Valencian. The task involves generating a commonsensical sentence that includes a given triplet of words. We evaluate a range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and Salamandra, on this benchmark. Our evaluation combines automatic metrics, LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human annotations. Results consistently show superior performance in English, with significantly lower performance in less-resourced languages. While contextual support yields mixed results, it tends to benefit underrepresented languages. These findings underscore the current limitations of LLMs in multilingual commonsense generation. The dataset is publicly available at https://huggingface.co/datasets/gplsi/MULTICOM.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in ~4.5K questions per language (~9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally 'deep' as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in commonsense reasoning; however, some variations in questions can trigger incorrect responses. Do these models truly understand commonsense knowledge, or just memorize expression patterns? To investigate this question, we present the first extensive robustness evaluation of LLMs in commonsense reasoning. We introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200 cases, by designing and compiling seven types of question variants. To construct this benchmark, we propose a two-stage method to develop Chinese HellaSwag, a finely annotated dataset comprising 12,000 instances across 56 categories. We conduct extensive experiments on 41 representative LLMs, revealing that these LLMs are far from robust in commonsense reasoning. Furthermore, this robustness varies depending on the language in which the LLM is tested. This work establishes a high-quality evaluation benchmark, with extensive experiments offering valuable insights to the community in commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2509.19265v1
Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World
Large language models (LLMs) often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. Although some work has explored cultural alignment, the potential for cross-cultural transfer, using alignment in one culture to improve performance in others, remains underexplored. This paper investigates cross-cultural transfer of commonsense reasoning in the Arab world, where linguistic and historical similarities coexist with local cultural differences. Using a culturally grounded commonsense reasoning dataset covering 13 Arab countries, we evaluate lightweight alignment methods such as in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization. Our results show that merely 12 culture-specific examples from one country can improve performance in others by 10\% on average, within multilingual models. In addition, we demonstrate that out-of-culture demonstrations from Indonesia and US contexts can match or surpass in-culture alignment for MCQ reasoning, highlighting cultural commonsense transferability beyond the Arab world. These findings demonstrate that efficient cross-cultural alignment is possible and offer a promising approach to adapt LLMs to low-resource cultural settings.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in answering questions pertaining to natural language inference and commonsense reasoning. Composing these questions is itself a skilled writing task, so in this paper we consider LLMs as authors of commonsense assessment items. We prompt LLMs to generate items in the style of a prominent benchmark for commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine the outcome according to analyses facilitated by the LLMs and human annotation. We find that LLMs that succeed in answering the original COPA benchmark are also more successful in authoring their own items.
<br>http://arxiv.org/abs/2509.18063v1
ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning
Large Language Models (LLMs) show strong reasoning abilities but rely on internalized knowledge that is often insufficient, outdated, or incorrect when trying to answer a question that requires specific domain knowledge. Knowledge Graphs (KGs) provide structured external knowledge, yet their complexity and multi-hop reasoning requirements make integration challenging. We present ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural language queries. We evaluate several not fine-tuned state-of-the art LLMs as backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and commonsense reasoning over long-tail entities. ARK-V1 achieves substantially higher conditional accuracies than Chain-of-Thought baselines, and larger backbone models show a clear trend toward better coverage, correctness, and stability.
<br>http://arxiv.org/abs/2509.06401v1
Do LLMs exhibit the same commonsense capabilities across languages?
This paper explores the multilingual commonsense generation abilities of Large Language Models (LLMs). To facilitate this investigation, we introduce MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four languages: English, Spanish, Dutch, and Valencian. The task involves generating a commonsensical sentence that includes a given triplet of words. We evaluate a range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and Salamandra, on this benchmark. Our evaluation combines automatic metrics, LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human annotations. Results consistently show superior performance in English, with significantly lower performance in less-resourced languages. While contextual support yields mixed results, it tends to benefit underrepresented languages. These findings underscore the current limitations of LLMs in multilingual commonsense generation. The dataset is publicly available at https://huggingface.co/datasets/gplsi/MULTICOM.
<br>http://arxiv.org/abs/2509.18063v1
ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning
Large Language Models (LLMs) show strong reasoning abilities but rely on internalized knowledge that is often insufficient, outdated, or incorrect when trying to answer a question that requires specific domain knowledge. Knowledge Graphs (KGs) provide structured external knowledge, yet their complexity and multi-hop reasoning requirements make integration challenging. We present ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural language queries. We evaluate several not fine-tuned state-of-the art LLMs as backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and commonsense reasoning over long-tail entities. ARK-V1 achieves substantially higher conditional accuracies than Chain-of-Thought baselines, and larger backbone models show a clear trend toward better coverage, correctness, and stability.
<br>http://arxiv.org/abs/2509.18063v1
ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning
Large Language Models (LLMs) show strong reasoning abilities but rely on internalized knowledge that is often insufficient, outdated, or incorrect when trying to answer a question that requires specific domain knowledge. Knowledge Graphs (KGs) provide structured external knowledge, yet their complexity and multi-hop reasoning requirements make integration challenging. We present ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural language queries. We evaluate several not fine-tuned state-of-the art LLMs as backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and commonsense reasoning over long-tail entities. ARK-V1 achieves substantially higher conditional accuracies than Chain-of-Thought baselines, and larger backbone models show a clear trend toward better coverage, correctness, and stability.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been made about their abilities, including for commonsense reasoning. Given the increasingly better results of current language models on previous static benchmarks for commonsense reasoning, we explore an alternative dialectical evaluation. The goal of this kind of evaluation is not to obtain an aggregate performance value but to find failures and map the boundaries of the system. Dialoguing with the system gives the opportunity to check for consistency and get more reassurance of these boundaries beyond anecdotal evidence. In this paper we conduct some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning). We conclude with some suggestions for future work both to improve the capabilities of language models and to systematise this kind of dialectical evaluation.
<br>http://arxiv.org/abs/2511.04087v1
E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce
Finding relevant products given a user query plays a pivotal role in an e-commerce platform, as it can spark shopping behaviors and result in revenue gains. The challenge lies in accurately predicting the correlation between queries and products. Recently, mining the cross-features between queries and products based on the commonsense reasoning capacity of Large Language Models (LLMs) has shown promising performance. However, such methods suffer from high costs due to intensive real-time LLM inference during serving, as well as human annotations and potential Supervised Fine Tuning (SFT). To boost efficiency while leveraging the commonsense reasoning capacity of LLMs for various e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation Enhancer (E-CARE). During inference, models augmented with E-CARE can access commonsense reasoning with only a single LLM forward pass per query by utilizing a commonsense reasoning factor graph that encodes most of the reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show an improvement of up to 12.1% on precision@5.
<br>http://arxiv.org/abs/2509.24468v1
Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset
Large language models (LLMs) exhibit social biases, prompting the development of various debiasing methods. However, debiasing methods may degrade the capabilities of LLMs. Previous research has evaluated the impact of bias mitigation primarily through tasks measuring general language understanding, which are often unrelated to social biases. In contrast, cultural commonsense is closely related to social biases, as both are rooted in social norms and values. The impact of bias mitigation on cultural commonsense in LLMs has not been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate social biases and cultural commonsense in LLMs in a unified format. We evaluate several LLMs on SOBACO to examine how debiasing methods affect cultural commonsense in LLMs. Our results reveal that the debiasing methods degrade the performance of the LLMs on the cultural commonsense task (up to 75% accuracy deterioration). These results highlight the importance of developing debiasing methods that consider the trade-off with cultural commonsense to improve fairness and utility of LLMs.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in ~4.5K questions per language (~9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally 'deep' as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Current protocols typically respond with immediate stops or minimal-risk maneuvers, worsening traffic flow and lacking flexibility for rare driving scenarios. In this paper, we propose LLM-RCO, a framework leveraging large language models to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator. These modules interact with the dynamic driving environment, enabling proactive and context-aware control actions to override the original control policy of autonomous agents. To improve safety in such challenging conditions, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, complete with annotations for LLM-based hazard inference and motion planning fine-tuning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that systems equipped with LLM-RCO significantly improve driving performance, highlighting its potential for enhancing autonomous driving resilience against adverse perception deficits. Our results also show that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Current protocols typically respond with immediate stops or minimal-risk maneuvers, worsening traffic flow and lacking flexibility for rare driving scenarios. In this paper, we propose LLM-RCO, a framework leveraging large language models to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator. These modules interact with the dynamic driving environment, enabling proactive and context-aware control actions to override the original control policy of autonomous agents. To improve safety in such challenging conditions, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, complete with annotations for LLM-based hazard inference and motion planning fine-tuning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that systems equipped with LLM-RCO significantly improve driving performance, highlighting its potential for enhancing autonomous driving resilience against adverse perception deficits. Our results also show that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2509.18063v1
ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning
Large Language Models (LLMs) show strong reasoning abilities but rely on internalized knowledge that is often insufficient, outdated, or incorrect when trying to answer a question that requires specific domain knowledge. Knowledge Graphs (KGs) provide structured external knowledge, yet their complexity and multi-hop reasoning requirements make integration challenging. We present ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural language queries. We evaluate several not fine-tuned state-of-the art LLMs as backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and commonsense reasoning over long-tail entities. ARK-V1 achieves substantially higher conditional accuracies than Chain-of-Thought baselines, and larger backbone models show a clear trend toward better coverage, correctness, and stability.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in ~4.5K questions per language (~9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally 'deep' as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in commonsense reasoning; however, some variations in questions can trigger incorrect responses. Do these models truly understand commonsense knowledge, or just memorize expression patterns? To investigate this question, we present the first extensive robustness evaluation of LLMs in commonsense reasoning. We introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200 cases, by designing and compiling seven types of question variants. To construct this benchmark, we propose a two-stage method to develop Chinese HellaSwag, a finely annotated dataset comprising 12,000 instances across 56 categories. We conduct extensive experiments on 41 representative LLMs, revealing that these LLMs are far from robust in commonsense reasoning. Furthermore, this robustness varies depending on the language in which the LLM is tested. This work establishes a high-quality evaluation benchmark, with extensive experiments offering valuable insights to the community in commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant commonsense persona knowledge, but retrieving this knowledge is challenging due to complex contexts and the implicit nature of commonsense. This paper presents our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge, addressing the critical need for integrating persona and commonsense knowledge in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that leverages Large Language Models to generate high-quality synthetic datasets for training commonsense persona knowledge linkers. To demonstrate the efficacy of our approach, we present SynCPKL, a new dataset specifically designed for this task. Our experiments validate the effectiveness of SynCPKL for training commonsense persona knowledge linkers. Additionally, our top-performing model, Derberta-SynCPKL, secured first place in the CPKL challenge by a 16% improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2509.18063v1
ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning
Large Language Models (LLMs) show strong reasoning abilities but rely on internalized knowledge that is often insufficient, outdated, or incorrect when trying to answer a question that requires specific domain knowledge. Knowledge Graphs (KGs) provide structured external knowledge, yet their complexity and multi-hop reasoning requirements make integration challenging. We present ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural language queries. We evaluate several not fine-tuned state-of-the art LLMs as backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and commonsense reasoning over long-tail entities. ARK-V1 achieves substantially higher conditional accuracies than Chain-of-Thought baselines, and larger backbone models show a clear trend toward better coverage, correctness, and stability.
<br>http://arxiv.org/abs/2503.07020v1
Combating Partial Perception Deficit in Autonomous Driving with Multimodal LLM Commonsense
Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Current protocols typically respond with immediate stops or minimal-risk maneuvers, worsening traffic flow and lacking flexibility for rare driving scenarios. In this paper, we propose LLM-RCO, a framework leveraging large language models to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator. These modules interact with the dynamic driving environment, enabling proactive and context-aware control actions to override the original control policy of autonomous agents. To improve safety in such challenging conditions, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, complete with annotations for LLM-based hazard inference and motion planning fine-tuning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that systems equipped with LLM-RCO significantly improve driving performance, highlighting its potential for enhancing autonomous driving resilience against adverse perception deficits. Our results also show that LLMs fine-tuned with DriveLM-Deficit can enable more proactive movements instead of conservative stops in the context of perception deficits.
<br>http://arxiv.org/abs/2509.06401v1
Do LLMs exhibit the same commonsense capabilities across languages?
This paper explores the multilingual commonsense generation abilities of Large Language Models (LLMs). To facilitate this investigation, we introduce MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four languages: English, Spanish, Dutch, and Valencian. The task involves generating a commonsensical sentence that includes a given triplet of words. We evaluate a range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and Salamandra, on this benchmark. Our evaluation combines automatic metrics, LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human annotations. Results consistently show superior performance in English, with significantly lower performance in less-resourced languages. While contextual support yields mixed results, it tends to benefit underrepresented languages. These findings underscore the current limitations of LLMs in multilingual commonsense generation. The dataset is publicly available at https://huggingface.co/datasets/gplsi/MULTICOM.
<br>http://arxiv.org/abs/2511.04087v1
E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce
Finding relevant products given a user query plays a pivotal role in an e-commerce platform, as it can spark shopping behaviors and result in revenue gains. The challenge lies in accurately predicting the correlation between queries and products. Recently, mining the cross-features between queries and products based on the commonsense reasoning capacity of Large Language Models (LLMs) has shown promising performance. However, such methods suffer from high costs due to intensive real-time LLM inference during serving, as well as human annotations and potential Supervised Fine Tuning (SFT). To boost efficiency while leveraging the commonsense reasoning capacity of LLMs for various e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation Enhancer (E-CARE). During inference, models augmented with E-CARE can access commonsense reasoning with only a single LLM forward pass per query by utilizing a commonsense reasoning factor graph that encodes most of the reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show an improvement of up to 12.1% on precision@5.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in ~4.5K questions per language (~9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally 'deep' as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in ~4.5K questions per language (~9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally 'deep' as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant commonsense persona knowledge, but retrieving this knowledge is challenging due to complex contexts and the implicit nature of commonsense. This paper presents our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge, addressing the critical need for integrating persona and commonsense knowledge in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that leverages Large Language Models to generate high-quality synthetic datasets for training commonsense persona knowledge linkers. To demonstrate the efficacy of our approach, we present SynCPKL, a new dataset specifically designed for this task. Our experiments validate the effectiveness of SynCPKL for training commonsense persona knowledge linkers. Additionally, our top-performing model, Derberta-SynCPKL, secured first place in the CPKL challenge by a 16% improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2509.24468v1
Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset
Large language models (LLMs) exhibit social biases, prompting the development of various debiasing methods. However, debiasing methods may degrade the capabilities of LLMs. Previous research has evaluated the impact of bias mitigation primarily through tasks measuring general language understanding, which are often unrelated to social biases. In contrast, cultural commonsense is closely related to social biases, as both are rooted in social norms and values. The impact of bias mitigation on cultural commonsense in LLMs has not been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate social biases and cultural commonsense in LLMs in a unified format. We evaluate several LLMs on SOBACO to examine how debiasing methods affect cultural commonsense in LLMs. Our results reveal that the debiasing methods degrade the performance of the LLMs on the cultural commonsense task (up to 75% accuracy deterioration). These results highlight the importance of developing debiasing methods that consider the trade-off with cultural commonsense to improve fairness and utility of LLMs.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant commonsense persona knowledge, but retrieving this knowledge is challenging due to complex contexts and the implicit nature of commonsense. This paper presents our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge, addressing the critical need for integrating persona and commonsense knowledge in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that leverages Large Language Models to generate high-quality synthetic datasets for training commonsense persona knowledge linkers. To demonstrate the efficacy of our approach, we present SynCPKL, a new dataset specifically designed for this task. Our experiments validate the effectiveness of SynCPKL for training commonsense persona knowledge linkers. Additionally, our top-performing model, Derberta-SynCPKL, secured first place in the CPKL challenge by a 16% improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2509.24468v1
Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset
Large language models (LLMs) exhibit social biases, prompting the development of various debiasing methods. However, debiasing methods may degrade the capabilities of LLMs. Previous research has evaluated the impact of bias mitigation primarily through tasks measuring general language understanding, which are often unrelated to social biases. In contrast, cultural commonsense is closely related to social biases, as both are rooted in social norms and values. The impact of bias mitigation on cultural commonsense in LLMs has not been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate social biases and cultural commonsense in LLMs in a unified format. We evaluate several LLMs on SOBACO to examine how debiasing methods affect cultural commonsense in LLMs. Our results reveal that the debiasing methods degrade the performance of the LLMs on the cultural commonsense task (up to 75% accuracy deterioration). These results highlight the importance of developing debiasing methods that consider the trade-off with cultural commonsense to improve fairness and utility of LLMs.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in commonsense reasoning; however, some variations in questions can trigger incorrect responses. Do these models truly understand commonsense knowledge, or just memorize expression patterns? To investigate this question, we present the first extensive robustness evaluation of LLMs in commonsense reasoning. We introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200 cases, by designing and compiling seven types of question variants. To construct this benchmark, we propose a two-stage method to develop Chinese HellaSwag, a finely annotated dataset comprising 12,000 instances across 56 categories. We conduct extensive experiments on 41 representative LLMs, revealing that these LLMs are far from robust in commonsense reasoning. Furthermore, this robustness varies depending on the language in which the LLM is tested. This work establishes a high-quality evaluation benchmark, with extensive experiments offering valuable insights to the community in commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in answering questions pertaining to natural language inference and commonsense reasoning. Composing these questions is itself a skilled writing task, so in this paper we consider LLMs as authors of commonsense assessment items. We prompt LLMs to generate items in the style of a prominent benchmark for commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine the outcome according to analyses facilitated by the LLMs and human annotation. We find that LLMs that succeed in answering the original COPA benchmark are also more successful in authoring their own items.
<br>http://arxiv.org/abs/2503.07020v2
Driving Through Uncertainty: Risk-Averse Control with LLM Commonsense for Autonomous Driving under Perception Deficits
Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Existing protocols typically default to entirely risk-avoidant actions such as immediate stops, which are detrimental to navigation goals and lack flexibility for rare driving scenarios. Yet, in cases of minor risk, halting the vehicle may be unnecessary, and more adaptive responses are preferable. In this paper, we propose LLM-RCO, a risk-averse framework leveraging large language models (LLMs) to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules interacting with the dynamic driving environment: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator, enabling proactive and context-aware actions in such challenging conditions. To enhance the driving decision-making of LLMs, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, annotated for LLM fine-tuning in hazard detection and motion planning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that LLM-RCO promotes proactive maneuvers over purely risk-averse actions in perception deficit scenarios, underscoring its value for boosting autonomous driving resilience against perception loss challenges.
<br>http://arxiv.org/abs/2511.04087v1
E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce
Finding relevant products given a user query plays a pivotal role in an e-commerce platform, as it can spark shopping behaviors and result in revenue gains. The challenge lies in accurately predicting the correlation between queries and products. Recently, mining the cross-features between queries and products based on the commonsense reasoning capacity of Large Language Models (LLMs) has shown promising performance. However, such methods suffer from high costs due to intensive real-time LLM inference during serving, as well as human annotations and potential Supervised Fine Tuning (SFT). To boost efficiency while leveraging the commonsense reasoning capacity of LLMs for various e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation Enhancer (E-CARE). During inference, models augmented with E-CARE can access commonsense reasoning with only a single LLM forward pass per query by utilizing a commonsense reasoning factor graph that encodes most of the reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show an improvement of up to 12.1% on precision@5.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in answering questions pertaining to natural language inference and commonsense reasoning. Composing these questions is itself a skilled writing task, so in this paper we consider LLMs as authors of commonsense assessment items. We prompt LLMs to generate items in the style of a prominent benchmark for commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine the outcome according to analyses facilitated by the LLMs and human annotation. We find that LLMs that succeed in answering the original COPA benchmark are also more successful in authoring their own items.
<br>http://arxiv.org/abs/2512.00069v1
Enhancing Cognitive Robotics with Commonsense through LLM-Generated Preconditions and Subgoals
Robots often fail at everyday tasks because instructions skip commonsense details like hidden preconditions and small subgoals. Traditional symbolic planners need these details to be written explicitly, which is time consuming and often incomplete. In this project we combine a Large Language Model with symbolic planning. Given a natural language task, the LLM suggests plausible preconditions and subgoals. We translate these suggestions into a formal planning model and execute the resulting plan in simulation. Compared to a baseline planner without the LLM step, our system produces more valid plans, achieves a higher task success rate, and adapts better when the environment changes. These results suggest that adding LLM commonsense to classical planning can make robot behavior in realistic scenarios more reliable.
<br>http://arxiv.org/abs/2509.06401v1
Do LLMs exhibit the same commonsense capabilities across languages?
This paper explores the multilingual commonsense generation abilities of Large Language Models (LLMs). To facilitate this investigation, we introduce MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four languages: English, Spanish, Dutch, and Valencian. The task involves generating a commonsensical sentence that includes a given triplet of words. We evaluate a range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and Salamandra, on this benchmark. Our evaluation combines automatic metrics, LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human annotations. Results consistently show superior performance in English, with significantly lower performance in less-resourced languages. While contextual support yields mixed results, it tends to benefit underrepresented languages. These findings underscore the current limitations of LLMs in multilingual commonsense generation. The dataset is publicly available at https://huggingface.co/datasets/gplsi/MULTICOM.
<br>http://arxiv.org/abs/2509.06401v1
Do LLMs exhibit the same commonsense capabilities across languages?
This paper explores the multilingual commonsense generation abilities of Large Language Models (LLMs). To facilitate this investigation, we introduce MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four languages: English, Spanish, Dutch, and Valencian. The task involves generating a commonsensical sentence that includes a given triplet of words. We evaluate a range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and Salamandra, on this benchmark. Our evaluation combines automatic metrics, LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human annotations. Results consistently show superior performance in English, with significantly lower performance in less-resourced languages. While contextual support yields mixed results, it tends to benefit underrepresented languages. These findings underscore the current limitations of LLMs in multilingual commonsense generation. The dataset is publicly available at https://huggingface.co/datasets/gplsi/MULTICOM.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and timeliness of large language models (LLMs) . However, the setting of this task overlooks a significant portion of commonsense knowledge based on free-text in the real world, characterized by broad knowledge scope, long content and non instantiation. The editing objects of previous methods (e.g., MEMIT) were single token or entity, which were not suitable for commonsense knowledge in free-text form. To address the aforementioned challenges, we conducted experiments from two perspectives: knowledge localization and knowledge editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT) method, revealing the challenges associated with the distribution of commonsense knowledge in MLP and Attention layers, as well as in decentralized distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which utilizes a Dynamics-aware Module to locate the parameter positions corresponding to commonsense knowledge, and uses Knowledge Editing Module to update knowledge. The DEM method fully explores the potential of the MLP and Attention layers, and successfully edits commonsense knowledge based on free-text. The experimental results indicate that the DEM can achieve excellent editing performance.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in answering questions pertaining to natural language inference and commonsense reasoning. Composing these questions is itself a skilled writing task, so in this paper we consider LLMs as authors of commonsense assessment items. We prompt LLMs to generate items in the style of a prominent benchmark for commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine the outcome according to analyses facilitated by the LLMs and human annotation. We find that LLMs that succeed in answering the original COPA benchmark are also more successful in authoring their own items.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in answering questions pertaining to natural language inference and commonsense reasoning. Composing these questions is itself a skilled writing task, so in this paper we consider LLMs as authors of commonsense assessment items. We prompt LLMs to generate items in the style of a prominent benchmark for commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine the outcome according to analyses facilitated by the LLMs and human annotation. We find that LLMs that succeed in answering the original COPA benchmark are also more successful in authoring their own items.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been made about their abilities, including for commonsense reasoning. Given the increasingly better results of current language models on previous static benchmarks for commonsense reasoning, we explore an alternative dialectical evaluation. The goal of this kind of evaluation is not to obtain an aggregate performance value but to find failures and map the boundaries of the system. Dialoguing with the system gives the opportunity to check for consistency and get more reassurance of these boundaries beyond anecdotal evidence. In this paper we conduct some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning). We conclude with some suggestions for future work both to improve the capabilities of language models and to systematise this kind of dialectical evaluation.
<br>http://arxiv.org/abs/2402.17302v3
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators, resulting in ~4.5K questions per language (~9K in total), making our dataset the largest of its kind. Our experiments show that automatic data adaptation from an existing English dataset is less effective for Sundanese. Interestingly, using the direct generation method on the target language, GPT-4 Turbo can generate questions with adequate general knowledge in both languages, albeit not as culturally 'deep' as humans. We also observe a higher occurrence of fluency errors in the Sundanese dataset, highlighting the discrepancy between medium- and lower-resource languages.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been made about their abilities, including for commonsense reasoning. Given the increasingly better results of current language models on previous static benchmarks for commonsense reasoning, we explore an alternative dialectical evaluation. The goal of this kind of evaluation is not to obtain an aggregate performance value but to find failures and map the boundaries of the system. Dialoguing with the system gives the opportunity to check for consistency and get more reassurance of these boundaries beyond anecdotal evidence. In this paper we conduct some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning). We conclude with some suggestions for future work both to improve the capabilities of language models and to systematise this kind of dialectical evaluation.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and timeliness of large language models (LLMs) . However, the setting of this task overlooks a significant portion of commonsense knowledge based on free-text in the real world, characterized by broad knowledge scope, long content and non instantiation. The editing objects of previous methods (e.g., MEMIT) were single token or entity, which were not suitable for commonsense knowledge in free-text form. To address the aforementioned challenges, we conducted experiments from two perspectives: knowledge localization and knowledge editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT) method, revealing the challenges associated with the distribution of commonsense knowledge in MLP and Attention layers, as well as in decentralized distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which utilizes a Dynamics-aware Module to locate the parameter positions corresponding to commonsense knowledge, and uses Knowledge Editing Module to update knowledge. The DEM method fully explores the potential of the MLP and Attention layers, and successfully edits commonsense knowledge based on free-text. The experimental results indicate that the DEM can achieve excellent editing performance.
<br>http://arxiv.org/abs/2509.18063v1
ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning
Large Language Models (LLMs) show strong reasoning abilities but rely on internalized knowledge that is often insufficient, outdated, or incorrect when trying to answer a question that requires specific domain knowledge. Knowledge Graphs (KGs) provide structured external knowledge, yet their complexity and multi-hop reasoning requirements make integration challenging. We present ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural language queries. We evaluate several not fine-tuned state-of-the art LLMs as backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and commonsense reasoning over long-tail entities. ARK-V1 achieves substantially higher conditional accuracies than Chain-of-Thought baselines, and larger backbone models show a clear trend toward better coverage, correctness, and stability.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in answering questions pertaining to natural language inference and commonsense reasoning. Composing these questions is itself a skilled writing task, so in this paper we consider LLMs as authors of commonsense assessment items. We prompt LLMs to generate items in the style of a prominent benchmark for commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine the outcome according to analyses facilitated by the LLMs and human annotation. We find that LLMs that succeed in answering the original COPA benchmark are also more successful in authoring their own items.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been made about their abilities, including for commonsense reasoning. Given the increasingly better results of current language models on previous static benchmarks for commonsense reasoning, we explore an alternative dialectical evaluation. The goal of this kind of evaluation is not to obtain an aggregate performance value but to find failures and map the boundaries of the system. Dialoguing with the system gives the opportunity to check for consistency and get more reassurance of these boundaries beyond anecdotal evidence. In this paper we conduct some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning). We conclude with some suggestions for future work both to improve the capabilities of language models and to systematise this kind of dialectical evaluation.
<br>http://arxiv.org/abs/2509.24468v1
Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset
Large language models (LLMs) exhibit social biases, prompting the development of various debiasing methods. However, debiasing methods may degrade the capabilities of LLMs. Previous research has evaluated the impact of bias mitigation primarily through tasks measuring general language understanding, which are often unrelated to social biases. In contrast, cultural commonsense is closely related to social biases, as both are rooted in social norms and values. The impact of bias mitigation on cultural commonsense in LLMs has not been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate social biases and cultural commonsense in LLMs in a unified format. We evaluate several LLMs on SOBACO to examine how debiasing methods affect cultural commonsense in LLMs. Our results reveal that the debiasing methods degrade the performance of the LLMs on the cultural commonsense task (up to 75% accuracy deterioration). These results highlight the importance of developing debiasing methods that consider the trade-off with cultural commonsense to improve fairness and utility of LLMs.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and timeliness of large language models (LLMs) . However, the setting of this task overlooks a significant portion of commonsense knowledge based on free-text in the real world, characterized by broad knowledge scope, long content and non instantiation. The editing objects of previous methods (e.g., MEMIT) were single token or entity, which were not suitable for commonsense knowledge in free-text form. To address the aforementioned challenges, we conducted experiments from two perspectives: knowledge localization and knowledge editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT) method, revealing the challenges associated with the distribution of commonsense knowledge in MLP and Attention layers, as well as in decentralized distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which utilizes a Dynamics-aware Module to locate the parameter positions corresponding to commonsense knowledge, and uses Knowledge Editing Module to update knowledge. The DEM method fully explores the potential of the MLP and Attention layers, and successfully edits commonsense knowledge based on free-text. The experimental results indicate that the DEM can achieve excellent editing performance.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in commonsense reasoning; however, some variations in questions can trigger incorrect responses. Do these models truly understand commonsense knowledge, or just memorize expression patterns? To investigate this question, we present the first extensive robustness evaluation of LLMs in commonsense reasoning. We introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200 cases, by designing and compiling seven types of question variants. To construct this benchmark, we propose a two-stage method to develop Chinese HellaSwag, a finely annotated dataset comprising 12,000 instances across 56 categories. We conduct extensive experiments on 41 representative LLMs, revealing that these LLMs are far from robust in commonsense reasoning. Furthermore, this robustness varies depending on the language in which the LLM is tested. This work establishes a high-quality evaluation benchmark, with extensive experiments offering valuable insights to the community in commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been made about their abilities, including for commonsense reasoning. Given the increasingly better results of current language models on previous static benchmarks for commonsense reasoning, we explore an alternative dialectical evaluation. The goal of this kind of evaluation is not to obtain an aggregate performance value but to find failures and map the boundaries of the system. Dialoguing with the system gives the opportunity to check for consistency and get more reassurance of these boundaries beyond anecdotal evidence. In this paper we conduct some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning). We conclude with some suggestions for future work both to improve the capabilities of language models and to systematise this kind of dialectical evaluation.
<br>http://arxiv.org/abs/2509.19265v1
Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World
Large language models (LLMs) often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. Although some work has explored cultural alignment, the potential for cross-cultural transfer, using alignment in one culture to improve performance in others, remains underexplored. This paper investigates cross-cultural transfer of commonsense reasoning in the Arab world, where linguistic and historical similarities coexist with local cultural differences. Using a culturally grounded commonsense reasoning dataset covering 13 Arab countries, we evaluate lightweight alignment methods such as in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization. Our results show that merely 12 culture-specific examples from one country can improve performance in others by 10\% on average, within multilingual models. In addition, we demonstrate that out-of-culture demonstrations from Indonesia and US contexts can match or surpass in-culture alignment for MCQ reasoning, highlighting cultural commonsense transferability beyond the Arab world. These findings demonstrate that efficient cross-cultural alignment is possible and offer a promising approach to adapt LLMs to low-resource cultural settings.
<br>http://arxiv.org/abs/2509.19265v1
Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World
Large language models (LLMs) often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. Although some work has explored cultural alignment, the potential for cross-cultural transfer, using alignment in one culture to improve performance in others, remains underexplored. This paper investigates cross-cultural transfer of commonsense reasoning in the Arab world, where linguistic and historical similarities coexist with local cultural differences. Using a culturally grounded commonsense reasoning dataset covering 13 Arab countries, we evaluate lightweight alignment methods such as in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization. Our results show that merely 12 culture-specific examples from one country can improve performance in others by 10\% on average, within multilingual models. In addition, we demonstrate that out-of-culture demonstrations from Indonesia and US contexts can match or surpass in-culture alignment for MCQ reasoning, highlighting cultural commonsense transferability beyond the Arab world. These findings demonstrate that efficient cross-cultural alignment is possible and offer a promising approach to adapt LLMs to low-resource cultural settings.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in answering questions pertaining to natural language inference and commonsense reasoning. Composing these questions is itself a skilled writing task, so in this paper we consider LLMs as authors of commonsense assessment items. We prompt LLMs to generate items in the style of a prominent benchmark for commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine the outcome according to analyses facilitated by the LLMs and human annotation. We find that LLMs that succeed in answering the original COPA benchmark are also more successful in authoring their own items.
<br>http://arxiv.org/abs/2403.14112v3
Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations
We introduce CHARM, the first benchmark for comprehensively and in-depth evaluating the commonsense reasoning ability of large language models (LLMs) in Chinese, which covers both globally known and Chinese-specific commonsense. We evaluated 7 English and 12 Chinese-oriented LLMs on CHARM, employing 5 representative prompt strategies for improving LLMs' reasoning ability, such as Chain-of-Thought. Our findings indicate that the LLM's language orientation and the task's domain influence the effectiveness of the prompt strategy, which enriches previous research findings. We built closely-interconnected reasoning and memorization tasks, and found that some LLMs struggle with memorizing Chinese commonsense, affecting their reasoning ability, while others show differences in reasoning despite similar memorization performance. We also evaluated the LLMs' memorization-independent reasoning abilities and analyzed the typical errors. Our study precisely identified the LLMs' strengths and weaknesses, providing the clear direction for optimization. It can also serve as a reference for studies in other fields. We will release CHARM at https://github.com/opendatalab/CHARM .
<br>http://arxiv.org/abs/2509.18063v1
ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning
Large Language Models (LLMs) show strong reasoning abilities but rely on internalized knowledge that is often insufficient, outdated, or incorrect when trying to answer a question that requires specific domain knowledge. Knowledge Graphs (KGs) provide structured external knowledge, yet their complexity and multi-hop reasoning requirements make integration challenging. We present ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural language queries. We evaluate several not fine-tuned state-of-the art LLMs as backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and commonsense reasoning over long-tail entities. ARK-V1 achieves substantially higher conditional accuracies than Chain-of-Thought baselines, and larger backbone models show a clear trend toward better coverage, correctness, and stability.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant commonsense persona knowledge, but retrieving this knowledge is challenging due to complex contexts and the implicit nature of commonsense. This paper presents our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge, addressing the critical need for integrating persona and commonsense knowledge in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that leverages Large Language Models to generate high-quality synthetic datasets for training commonsense persona knowledge linkers. To demonstrate the efficacy of our approach, we present SynCPKL, a new dataset specifically designed for this task. Our experiments validate the effectiveness of SynCPKL for training commonsense persona knowledge linkers. Additionally, our top-performing model, Derberta-SynCPKL, secured first place in the CPKL challenge by a 16% improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2407.15281v1
SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking
Understanding rich dialogues often requires NLP systems to access relevant commonsense persona knowledge, but retrieving this knowledge is challenging due to complex contexts and the implicit nature of commonsense. This paper presents our approach to the Commonsense Persona Knowledge Linking (CPKL) challenge, addressing the critical need for integrating persona and commonsense knowledge in open-domain dialogue systems. We introduce SynCPKL Pipeline, a pipeline that leverages Large Language Models to generate high-quality synthetic datasets for training commonsense persona knowledge linkers. To demonstrate the efficacy of our approach, we present SynCPKL, a new dataset specifically designed for this task. Our experiments validate the effectiveness of SynCPKL for training commonsense persona knowledge linkers. Additionally, our top-performing model, Derberta-SynCPKL, secured first place in the CPKL challenge by a 16% improvement in F1 score. We released both SynCPKL and Derberta-SynCPKL at https://github.com/irislin1006/CPKL.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in commonsense reasoning; however, some variations in questions can trigger incorrect responses. Do these models truly understand commonsense knowledge, or just memorize expression patterns? To investigate this question, we present the first extensive robustness evaluation of LLMs in commonsense reasoning. We introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200 cases, by designing and compiling seven types of question variants. To construct this benchmark, we propose a two-stage method to develop Chinese HellaSwag, a finely annotated dataset comprising 12,000 instances across 56 categories. We conduct extensive experiments on 41 representative LLMs, revealing that these LLMs are far from robust in commonsense reasoning. Furthermore, this robustness varies depending on the language in which the LLM is tested. This work establishes a high-quality evaluation benchmark, with extensive experiments offering valuable insights to the community in commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and timeliness of large language models (LLMs) . However, the setting of this task overlooks a significant portion of commonsense knowledge based on free-text in the real world, characterized by broad knowledge scope, long content and non instantiation. The editing objects of previous methods (e.g., MEMIT) were single token or entity, which were not suitable for commonsense knowledge in free-text form. To address the aforementioned challenges, we conducted experiments from two perspectives: knowledge localization and knowledge editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT) method, revealing the challenges associated with the distribution of commonsense knowledge in MLP and Attention layers, as well as in decentralized distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which utilizes a Dynamics-aware Module to locate the parameter positions corresponding to commonsense knowledge, and uses Knowledge Editing Module to update knowledge. The DEM method fully explores the potential of the MLP and Attention layers, and successfully edits commonsense knowledge based on free-text. The experimental results indicate that the DEM can achieve excellent editing performance.
<br>http://arxiv.org/abs/2511.04087v1
E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce
Finding relevant products given a user query plays a pivotal role in an e-commerce platform, as it can spark shopping behaviors and result in revenue gains. The challenge lies in accurately predicting the correlation between queries and products. Recently, mining the cross-features between queries and products based on the commonsense reasoning capacity of Large Language Models (LLMs) has shown promising performance. However, such methods suffer from high costs due to intensive real-time LLM inference during serving, as well as human annotations and potential Supervised Fine Tuning (SFT). To boost efficiency while leveraging the commonsense reasoning capacity of LLMs for various e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation Enhancer (E-CARE). During inference, models augmented with E-CARE can access commonsense reasoning with only a single LLM forward pass per query by utilizing a commonsense reasoning factor graph that encodes most of the reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show an improvement of up to 12.1% on precision@5.
<br>http://arxiv.org/abs/2503.07020v2
Driving Through Uncertainty: Risk-Averse Control with LLM Commonsense for Autonomous Driving under Perception Deficits
Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Existing protocols typically default to entirely risk-avoidant actions such as immediate stops, which are detrimental to navigation goals and lack flexibility for rare driving scenarios. Yet, in cases of minor risk, halting the vehicle may be unnecessary, and more adaptive responses are preferable. In this paper, we propose LLM-RCO, a risk-averse framework leveraging large language models (LLMs) to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules interacting with the dynamic driving environment: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator, enabling proactive and context-aware actions in such challenging conditions. To enhance the driving decision-making of LLMs, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, annotated for LLM fine-tuning in hazard detection and motion planning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that LLM-RCO promotes proactive maneuvers over purely risk-averse actions in perception deficit scenarios, underscoring its value for boosting autonomous driving resilience against perception loss challenges.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been made about their abilities, including for commonsense reasoning. Given the increasingly better results of current language models on previous static benchmarks for commonsense reasoning, we explore an alternative dialectical evaluation. The goal of this kind of evaluation is not to obtain an aggregate performance value but to find failures and map the boundaries of the system. Dialoguing with the system gives the opportunity to check for consistency and get more reassurance of these boundaries beyond anecdotal evidence. In this paper we conduct some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning). We conclude with some suggestions for future work both to improve the capabilities of language models and to systematise this kind of dialectical evaluation.
<br>http://arxiv.org/abs/2502.11393v2
HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning
Large language models (LLMs) have shown remarkable capabilities in commonsense reasoning; however, some variations in questions can trigger incorrect responses. Do these models truly understand commonsense knowledge, or just memorize expression patterns? To investigate this question, we present the first extensive robustness evaluation of LLMs in commonsense reasoning. We introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200 cases, by designing and compiling seven types of question variants. To construct this benchmark, we propose a two-stage method to develop Chinese HellaSwag, a finely annotated dataset comprising 12,000 instances across 56 categories. We conduct extensive experiments on 41 representative LLMs, revealing that these LLMs are far from robust in commonsense reasoning. Furthermore, this robustness varies depending on the language in which the LLM is tested. This work establishes a high-quality evaluation benchmark, with extensive experiments offering valuable insights to the community in commonsense reasoning for LLMs.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been made about their abilities, including for commonsense reasoning. Given the increasingly better results of current language models on previous static benchmarks for commonsense reasoning, we explore an alternative dialectical evaluation. The goal of this kind of evaluation is not to obtain an aggregate performance value but to find failures and map the boundaries of the system. Dialoguing with the system gives the opportunity to check for consistency and get more reassurance of these boundaries beyond anecdotal evidence. In this paper we conduct some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning). We conclude with some suggestions for future work both to improve the capabilities of language models and to systematise this kind of dialectical evaluation.
<br>http://arxiv.org/abs/2509.24468v1
Bias Mitigation or Cultural Commonsense? Evaluating LLMs with a Japanese Dataset
Large language models (LLMs) exhibit social biases, prompting the development of various debiasing methods. However, debiasing methods may degrade the capabilities of LLMs. Previous research has evaluated the impact of bias mitigation primarily through tasks measuring general language understanding, which are often unrelated to social biases. In contrast, cultural commonsense is closely related to social biases, as both are rooted in social norms and values. The impact of bias mitigation on cultural commonsense in LLMs has not been well investigated. Considering this gap, we propose SOBACO (SOcial BiAs and Cultural cOmmonsense benchmark), a Japanese benchmark designed to evaluate social biases and cultural commonsense in LLMs in a unified format. We evaluate several LLMs on SOBACO to examine how debiasing methods affect cultural commonsense in LLMs. Our results reveal that the debiasing methods degrade the performance of the LLMs on the cultural commonsense task (up to 75% accuracy deterioration). These results highlight the importance of developing debiasing methods that consider the trade-off with cultural commonsense to improve fairness and utility of LLMs.
<br>http://arxiv.org/abs/2410.14897v1
From Test-Taking to Test-Making: Examining LLM Authoring of Commonsense Assessment Items
LLMs can now perform a variety of complex writing tasks. They also excel in answering questions pertaining to natural language inference and commonsense reasoning. Composing these questions is itself a skilled writing task, so in this paper we consider LLMs as authors of commonsense assessment items. We prompt LLMs to generate items in the style of a prominent benchmark for commonsense reasoning, the Choice of Plausible Alternatives (COPA). We examine the outcome according to analyses facilitated by the LLMs and human annotation. We find that LLMs that succeed in answering the original COPA benchmark are also more successful in authoring their own items.
<br>http://arxiv.org/abs/2503.07020v2
Driving Through Uncertainty: Risk-Averse Control with LLM Commonsense for Autonomous Driving under Perception Deficits
Partial perception deficits can compromise autonomous vehicle safety by disrupting environmental understanding. Existing protocols typically default to entirely risk-avoidant actions such as immediate stops, which are detrimental to navigation goals and lack flexibility for rare driving scenarios. Yet, in cases of minor risk, halting the vehicle may be unnecessary, and more adaptive responses are preferable. In this paper, we propose LLM-RCO, a risk-averse framework leveraging large language models (LLMs) to integrate human-like driving commonsense into autonomous systems facing perception deficits. LLM-RCO features four key modules interacting with the dynamic driving environment: hazard inference, short-term motion planner, action condition verifier, and safety constraint generator, enabling proactive and context-aware actions in such challenging conditions. To enhance the driving decision-making of LLMs, we construct DriveLM-Deficit, a dataset of 53,895 video clips featuring deficits of safety-critical objects, annotated for LLM fine-tuning in hazard detection and motion planning. Extensive experiments in adverse driving conditions with the CARLA simulator demonstrate that LLM-RCO promotes proactive maneuvers over purely risk-averse actions in perception deficit scenarios, underscoring its value for boosting autonomous driving resilience against perception loss challenges.
<br>http://arxiv.org/abs/2511.04087v1
E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce
Finding relevant products given a user query plays a pivotal role in an e-commerce platform, as it can spark shopping behaviors and result in revenue gains. The challenge lies in accurately predicting the correlation between queries and products. Recently, mining the cross-features between queries and products based on the commonsense reasoning capacity of Large Language Models (LLMs) has shown promising performance. However, such methods suffer from high costs due to intensive real-time LLM inference during serving, as well as human annotations and potential Supervised Fine Tuning (SFT). To boost efficiency while leveraging the commonsense reasoning capacity of LLMs for various e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation Enhancer (E-CARE). During inference, models augmented with E-CARE can access commonsense reasoning with only a single LLM forward pass per query by utilizing a commonsense reasoning factor graph that encodes most of the reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show an improvement of up to 12.1% on precision@5.
<br>http://arxiv.org/abs/2601.17971v1
LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction
Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.
<br>http://arxiv.org/abs/2509.19265v1
Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World
Large language models (LLMs) often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. Although some work has explored cultural alignment, the potential for cross-cultural transfer, using alignment in one culture to improve performance in others, remains underexplored. This paper investigates cross-cultural transfer of commonsense reasoning in the Arab world, where linguistic and historical similarities coexist with local cultural differences. Using a culturally grounded commonsense reasoning dataset covering 13 Arab countries, we evaluate lightweight alignment methods such as in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization. Our results show that merely 12 culture-specific examples from one country can improve performance in others by 10\% on average, within multilingual models. In addition, we demonstrate that out-of-culture demonstrations from Indonesia and US contexts can match or surpass in-culture alignment for MCQ reasoning, highlighting cultural commonsense transferability beyond the Arab world. These findings demonstrate that efficient cross-cultural alignment is possible and offer a promising approach to adapt LLMs to low-resource cultural settings.
<br>http://arxiv.org/abs/2512.00069v1
Enhancing Cognitive Robotics with Commonsense through LLM-Generated Preconditions and Subgoals
Robots often fail at everyday tasks because instructions skip commonsense details like hidden preconditions and small subgoals. Traditional symbolic planners need these details to be written explicitly, which is time consuming and often incomplete. In this project we combine a Large Language Model with symbolic planning. Given a natural language task, the LLM suggests plausible preconditions and subgoals. We translate these suggestions into a formal planning model and execute the resulting plan in simulation. Compared to a baseline planner without the LLM step, our system produces more valid plans, achieves a higher task success rate, and adapts better when the environment changes. These results suggest that adding LLM commonsense to classical planning can make robot behavior in realistic scenarios more reliable.
<br>http://arxiv.org/abs/2509.19265v1
Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World
Large language models (LLMs) often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. Although some work has explored cultural alignment, the potential for cross-cultural transfer, using alignment in one culture to improve performance in others, remains underexplored. This paper investigates cross-cultural transfer of commonsense reasoning in the Arab world, where linguistic and historical similarities coexist with local cultural differences. Using a culturally grounded commonsense reasoning dataset covering 13 Arab countries, we evaluate lightweight alignment methods such as in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization. Our results show that merely 12 culture-specific examples from one country can improve performance in others by 10\% on average, within multilingual models. In addition, we demonstrate that out-of-culture demonstrations from Indonesia and US contexts can match or surpass in-culture alignment for MCQ reasoning, highlighting cultural commonsense transferability beyond the Arab world. These findings demonstrate that efficient cross-cultural alignment is possible and offer a promising approach to adapt LLMs to low-resource cultural settings.
<br>http://arxiv.org/abs/2512.00069v1
Enhancing Cognitive Robotics with Commonsense through LLM-Generated Preconditions and Subgoals
Robots often fail at everyday tasks because instructions skip commonsense details like hidden preconditions and small subgoals. Traditional symbolic planners need these details to be written explicitly, which is time consuming and often incomplete. In this project we combine a Large Language Model with symbolic planning. Given a natural language task, the LLM suggests plausible preconditions and subgoals. We translate these suggestions into a formal planning model and execute the resulting plan in simulation. Compared to a baseline planner without the LLM step, our system produces more valid plans, achieves a higher task success rate, and adapts better when the environment changes. These results suggest that adding LLM commonsense to classical planning can make robot behavior in realistic scenarios more reliable.
<br>http://arxiv.org/abs/2512.00069v1
Enhancing Cognitive Robotics with Commonsense through LLM-Generated Preconditions and Subgoals
Robots often fail at everyday tasks because instructions skip commonsense details like hidden preconditions and small subgoals. Traditional symbolic planners need these details to be written explicitly, which is time consuming and often incomplete. In this project we combine a Large Language Model with symbolic planning. Given a natural language task, the LLM suggests plausible preconditions and subgoals. We translate these suggestions into a formal planning model and execute the resulting plan in simulation. Compared to a baseline planner without the LLM step, our system produces more valid plans, achieves a higher task success rate, and adapts better when the environment changes. These results suggest that adding LLM commonsense to classical planning can make robot behavior in realistic scenarios more reliable.
<br>http://arxiv.org/abs/2601.17971v1
LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction
Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.
<br>http://arxiv.org/abs/2511.04087v1
E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce
Finding relevant products given a user query plays a pivotal role in an e-commerce platform, as it can spark shopping behaviors and result in revenue gains. The challenge lies in accurately predicting the correlation between queries and products. Recently, mining the cross-features between queries and products based on the commonsense reasoning capacity of Large Language Models (LLMs) has shown promising performance. However, such methods suffer from high costs due to intensive real-time LLM inference during serving, as well as human annotations and potential Supervised Fine Tuning (SFT). To boost efficiency while leveraging the commonsense reasoning capacity of LLMs for various e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation Enhancer (E-CARE). During inference, models augmented with E-CARE can access commonsense reasoning with only a single LLM forward pass per query by utilizing a commonsense reasoning factor graph that encodes most of the reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show an improvement of up to 12.1% on precision@5.
<br>http://arxiv.org/abs/2511.04087v1
E-CARE: An Efficient LLM-based Commonsense-Augmented Framework for E-Commerce
Finding relevant products given a user query plays a pivotal role in an e-commerce platform, as it can spark shopping behaviors and result in revenue gains. The challenge lies in accurately predicting the correlation between queries and products. Recently, mining the cross-features between queries and products based on the commonsense reasoning capacity of Large Language Models (LLMs) has shown promising performance. However, such methods suffer from high costs due to intensive real-time LLM inference during serving, as well as human annotations and potential Supervised Fine Tuning (SFT). To boost efficiency while leveraging the commonsense reasoning capacity of LLMs for various e-commerce tasks, we propose the Efficient Commonsense-Augmented Recommendation Enhancer (E-CARE). During inference, models augmented with E-CARE can access commonsense reasoning with only a single LLM forward pass per query by utilizing a commonsense reasoning factor graph that encodes most of the reasoning schema from powerful LLMs. The experiments on 2 downstream tasks show an improvement of up to 12.1% on precision@5.
<br>http://arxiv.org/abs/2601.17971v1
LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction
Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.
<br>http://arxiv.org/abs/2410.23844v1
Commonsense Knowledge Editing Based on Free-Text in LLMs
Knowledge editing technology is crucial for maintaining the accuracy and timeliness of large language models (LLMs) . However, the setting of this task overlooks a significant portion of commonsense knowledge based on free-text in the real world, characterized by broad knowledge scope, long content and non instantiation. The editing objects of previous methods (e.g., MEMIT) were single token or entity, which were not suitable for commonsense knowledge in free-text form. To address the aforementioned challenges, we conducted experiments from two perspectives: knowledge localization and knowledge editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT) method, revealing the challenges associated with the distribution of commonsense knowledge in MLP and Attention layers, as well as in decentralized distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which utilizes a Dynamics-aware Module to locate the parameter positions corresponding to commonsense knowledge, and uses Knowledge Editing Module to update knowledge. The DEM method fully explores the potential of the MLP and Attention layers, and successfully edits commonsense knowledge based on free-text. The experimental results indicate that the DEM can achieve excellent editing performance.
<br>http://arxiv.org/abs/2601.17971v1
LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction
Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.
<br>http://arxiv.org/abs/2304.11164v1
Dialectical language model evaluation: An initial appraisal of the commonsense spatial reasoning abilities of LLMs
Language models have become very popular recently and many claims have been made about their abilities, including for commonsense reasoning. Given the increasingly better results of current language models on previous static benchmarks for commonsense reasoning, we explore an alternative dialectical evaluation. The goal of this kind of evaluation is not to obtain an aggregate performance value but to find failures and map the boundaries of the system. Dialoguing with the system gives the opportunity to check for consistency and get more reassurance of these boundaries beyond anecdotal evidence. In this paper we conduct some qualitative investigations of this kind of evaluation for the particular case of spatial reasoning (which is a fundamental aspect of commonsense reasoning). We conclude with some suggestions for future work both to improve the capabilities of language models and to systematise this kind of dialectical evaluation.
<br>http://arxiv.org/abs/2601.17971v1
LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction
Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.
<br>http://arxiv.org/abs/2509.18063v1
ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning
Large Language Models (LLMs) show strong reasoning abilities but rely on internalized knowledge that is often insufficient, outdated, or incorrect when trying to answer a question that requires specific domain knowledge. Knowledge Graphs (KGs) provide structured external knowledge, yet their complexity and multi-hop reasoning requirements make integration challenging. We present ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural language queries. We evaluate several not fine-tuned state-of-the art LLMs as backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and commonsense reasoning over long-tail entities. ARK-V1 achieves substantially higher conditional accuracies than Chain-of-Thought baselines, and larger backbone models show a clear trend toward better coverage, correctness, and stability.
<br>http://arxiv.org/abs/2601.17971v1
LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction
Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.
